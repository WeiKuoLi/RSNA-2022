{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"#  ****A taste of this competition, however inorder to do real work I need more RAM. Start trying google cloud AI service. First thing to do: link the database to google cloud service****\n1. Upload kaggle dataset from a kaggle notebook\n2. Use ADD DATA function to select competition database","metadata":{}},{"cell_type":"code","source":"import os \nfrom google.cloud import storage\n \nGCP_PROJECT_ID='rsna2022-363105'\nstorage_client = storage.Client(project=GCP_PROJECT_ID)\ndef create_bucket(bucket_name):\n bucket = storage_client.create_bucket(bucket_name)\ndef upload_files(bucket_name, source_folder):\n bucket = storage_client.get_bucket(bucket_name)\n for filename in os.listdir(source_folder):\n  blob = bucket.blob(filename)\n  blob.upload_from_filename(source_folder + filename)\n#print(\"DONE\")\n \nbucket_name = GCP_PROJECT_ID+'_1'\ncreate_bucket(bucket_name)\n\nlocal_data = '../input/for-pydicom/'\nos.listdir(local_data)\nupload_files(bucket_name,local_data) ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The code above does not work because the function can only upload files not folders.","metadata":{}},{"cell_type":"code","source":"import glob \n \ndef upload_local_directory_to_gcs(local_path, bucket_name, gcs_path):\n    assert os.path.isdir(local_path)\n    bucket = storage_client.get_bucket(bucket_name)\n    for local_file in glob.glob(local_path + '/**'):\n        if not os.path.isfile(local_file):\n           upload_local_directory_to_gcs(local_file, bucket, gcs_path + \"/\" + os.path.basename(local_file))\n        else:\n           remote_path = os.path.join(gcs_path, local_file[1 + len(local_path):])\n           blob = bucket.blob(remote_path)\n           blob.upload_from_filename(local_file)\n        \nlocal_data = '../input/for-pydicom'\nupload_local_directory_to_gcs(local_data, bucket_name, 'input/for-pydicom')\n\n\n!ls ../input/*py*\n\nlocal_data = '../input/rsna-2022-cervical-spine-fracture-detection'\nupload_local_directory_to_gcs(local_data, bucket_name, 'input/rsna-2022-cervical-spine-fracture-detection')","metadata":{},"execution_count":null,"outputs":[]}]}